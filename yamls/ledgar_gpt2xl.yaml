# works with experiments/ledgar.py, not MCLI

model_name_or_path: gpt2-xl
do_lower_case: False
output_dir: logs/ledgar/gpt2-xl/seed_1
do_train: True
do_eval: True
do_predict: True
overwrite_output_dir: True
load_best_model_at_end: True
metric_for_best_model: micro-f1
greater_is_better: True
evaluation_strategy: epoch
save_strategy: epoch
save_total_limit: 5
num_train_epochs: 1
# yaml converts scientific notation to str...
learning_rate: 0.000006
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
seed: 1
fp16: True
fp16_full_eval: True
max_seq_length: 512
gradient_checkpointing: True
max_train_samples: 1000 # just for testing
pad_to_max_length: False